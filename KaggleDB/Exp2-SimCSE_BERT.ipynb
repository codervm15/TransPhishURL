{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415e221-2058-4108-9481-486e18f705c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06a7c3-02e1-4718-b2e7-5b2f9df3089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "print('Device name:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b8710-8608-4382-ba41-1f35b8750549",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/vikrant/Desktop/Thesis/Thesis_Projects/URL_detection/Kaggle/phishing_site_urls.csv\")\n",
    "\n",
    "label_map = {\"good\": 0, \"bad\": 1}\n",
    "data['label'] = data['Label'].map(label_map)\n",
    "\n",
    "# Count unique occurrences of 0s and 1s in the dataset\n",
    "label_counts = data['label'].value_counts()\n",
    "print(\"Counts of unique labels:\")\n",
    "print(label_counts)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9ff56-c448-4022-aae5-6cfc662e375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15da9b-86ca-46de-ba06-d937df340b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626251ee-e5de-4f75-bd29-5e53156f4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['URL'].values\n",
    "labels = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597ee51-ae76-4647-81df-a2f35d67ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('princeton-nlp/sup-simcse-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6086b4-4722-4349-bf2f-c67db4c25085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "max_length = 128  # Adjusted sequence length for better performance\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_length,\n",
    "                        pad_to_max_length=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4177dbb-8428-4bb6-b1ac-e5fa4e410fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 80% training and 20% test set\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels,\n",
    "                                                                        random_state=2018, test_size=0.2)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks, attention_masks,\n",
    "                                                 random_state=2018, test_size=0.2)\n",
    "\n",
    "# Further split the training data into 75% training and 25% validation\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_inputs, train_labels,\n",
    "                                                                                    random_state=2018, test_size=0.25)\n",
    "train_masks, validation_masks, _, _ = train_test_split(train_masks, train_masks,\n",
    "                                                       random_state=2018, test_size=0.25)\n",
    "\n",
    "# Create the DataLoader for training, validation, and test sets\n",
    "batch_size_train = 32\n",
    "batch_size_val = 64\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size_train, num_workers=16)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size_val, num_workers=16)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size_val, num_workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b3842-4f64-4e51-9855-dd305d86f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"princeton-nlp/sup-simcse-roberta-large\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df5d49-60e6-4a4a-a136-c82208396c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037f373-15c0-4840-be22-b00b0089856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping criteria\n",
    "patience = 2\n",
    "early_stopping_counter = 0\n",
    "best_validation_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686571a-6f50-482e-b9bf-d8f2fa331c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(f'Epoch {epoch_i + 1}/{epochs}')\n",
    "    print('Training...')\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Shuffle the training data at the beginning of each epoch\n",
    "    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size_train)\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(f'Average training loss: {avg_train_loss}')\n",
    "\n",
    "    print('Running Validation...')\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_loss = outputs.loss.item()  # Adjusted this line to correctly access loss\n",
    "        eval_loss += tmp_eval_loss\n",
    "\n",
    "        preds_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        eval_accuracy += np.sum(preds_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    avg_val_accuracy = eval_accuracy / nb_eval_steps\n",
    "    avg_val_loss = eval_loss / nb_eval_steps\n",
    "    validation_loss_values.append(avg_val_loss)\n",
    "\n",
    "    print(f'Validation Accuracy: {avg_val_accuracy}')\n",
    "    print(f'Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_validation_loss:\n",
    "        best_validation_loss = avg_val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print('Early stopping triggered.')\n",
    "        break\n",
    "\n",
    "print(f'Training complete!')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    predictions.extend(np.argmax(logits, axis=1).flatten())\n",
    "    true_labels.extend(label_ids.flatten())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predictions, digits=4))\n",
    "\n",
    "# Plot the learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e045b2-0798-4f9b-9370-cca9e4fab96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
